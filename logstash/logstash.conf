input {
  beats {
    port => 5044
  }
}

filter {
  # if message is JSON, parse it to structured fields
  json {
    source => "message"
    target => "json_payload"
    skip_on_invalid_json => true
  }

  # if we parsed JSON, copy some common nested fields to root for ease of use
  if [json_payload] {
    if [json_payload][level] {
      mutate { add_field => { "log_level" => "%{[json_payload][level]}" } }
    }
    if [json_payload][message] {
      mutate { add_field => { "message" => "%{[json_payload][message]}" } }
    }
    # map nested user/request ids if present
    if [json_payload][user] and [json_payload][user][id] {
      mutate { add_field => { "user.id" => "%{[json_payload][user][id]}" } }
    }
    if [json_payload][request] and [json_payload][request][id] {
      mutate { add_field => { "request.id" => "%{[json_payload][request][id]}" } }
    }
  }
  # try several grok patterns to be robust to different Python/log formats
  grok {
    match => {
      "message" => [
        "%{TIMESTAMP_ISO8601:log_timestamp} %{LOGLEVEL:log_level} %{GREEDYDATA:log_message}",
        "%{TIMESTAMP_ISO8601:log_timestamp} - %{DATA:module} - %{LOGLEVEL:log_level} - %{GREEDYDATA:log_message}",
        "\[%{TIMESTAMP_ISO8601:log_timestamp}\] %{LOGLEVEL:log_level}: %{GREEDYDATA:log_message}",
        "%{GREEDYDATA:log_message}"
      ]
    }
  overwrite => ["message"]
  }

  # capture exception traces as a separate field if present
  if [log_message] =~ /Traceback/ {
    mutate {
      add_tag => ["exception"]
    }
  }

  date {
    match => ["log_timestamp", "ISO8601"]
    target => "@timestamp"
    # if parsing fails, keep event timestamp
    remove_field => ["log_timestamp"]
  }

  mutate {
    add_field => { "[@metadata][index_prefix]" => "app-logs" }
    rename => { "log_message" => "message" }
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "app-logs-%{+YYYY.MM.dd}"
  }
  stdout { codec => rubydebug }
}
